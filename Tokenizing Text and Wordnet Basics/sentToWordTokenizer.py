from nltk.tokenize import word_tokenize
word_tokenize("Hello, guys how are you?")


from nltk.tokenize import 

# following are the different tokenizers
# Basically they are used to tokenize the words 
# from sentences.
#  
#1) PunktWordTokenizer 
# It splits on punctuation, but keeps it with the word instead of creating separate tokens
# Currently gives error on importing

#2) TreebankTokenizer
# (Basically tokenizes the words form each sentences even the punctuation are seperated)
# - from nltk.tokenize import word_tokenize


#3) RegexpTokenizer
#4)WordPunctTokenizer
#5)WhiteSpaceTokenizer

